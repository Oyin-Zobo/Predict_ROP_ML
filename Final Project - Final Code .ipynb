{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import warnings\n",
    "#import stats\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Final 12.25 without flow parameters.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'table.xlsx', sheet_name='sheet1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_data = data.replace (-9999, 0)\n",
    "#cleaner = cleaner_data(1:24340, 1:14)\n",
    "#data_1 = data(:,[1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = print(cleaner_data[cleaner_data == 0].count(axis=0)/len(cleaner_data.index))\n",
    "\n",
    "y = pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner_data[['Depth','ecd','GR', 'Res B','Res C','Res D','Rop','Rpm','Sppa','flow','tor','wob','JIF','HSI','Mse']]\n",
    "#print(data)\n",
    "#data.replace (-9999, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.hist(figsize=(8,7),column= ['ecd','GR', 'Res B','Res C','Res D','Rop','Rpm','Sppa','tor','flow','wob','JIF','HSI','Mse']);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = cleaner_data[cleaner_data['wob']>0]\n",
    "data_1 = data_5[(data_5['HSI'] >0)]\n",
    "data_2 = data_1[(data_1['Res B'] >0)]\n",
    "data_21 = data_2[(data_2['Res C'] >0)]\n",
    "data_31 = data_21[(data_21['Res D'] >0)]\n",
    "data_3 = data_31[(data_31['Rpm'] >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.hist(figsize=(8,7),column= ['ecd','GR', 'Res B','Res C','Res D','Rop','Rpm','Sppa','tor','flow','wob','JIF','HSI','Mse']);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_data, fitted_lambda = stats.boxcox(clean['Res B'])\n",
    "clean['Res B'] = fitted_data\n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "print(fitted_lambda)\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(clean['Res D'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])\n",
    "\n",
    "\n",
    "\n",
    "# adding legends to the subplots\n",
    "plt.legend(loc = \"upper right\")\n",
    "  \n",
    "# rescaling the subplots\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "  \n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_data, fitted_lambda = stats.boxcox(clean['Res C'])\n",
    "clean['Res C'] = fitted_data\n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "print(fitted_lambda)\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(clean['Res C'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])\n",
    "\n",
    "# adding legends to the subplots\n",
    "plt.legend(loc = \"upper right\")\n",
    "  \n",
    "# rescaling the subplots\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "  \n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_data, fitted_lambda = stats.boxcox(clean['Res D'])\n",
    "clean['Res D'] = fitted_data\n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "print(fitted_lambda)\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(clean['Res D'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])\n",
    "# adding legends to the subplots\n",
    "plt.legend(loc = \"upper right\")\n",
    "  \n",
    "# rescaling the subplots\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "  \n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_data, fitted_lambda = stats.boxcox(clean['Mse'])\n",
    "clean['Mse'] = fitted_data\n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "print(fitted_lambda)\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(clean['Mse'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])\n",
    "\n",
    "# adding legends to the subplots\n",
    "plt.legend(loc = \"upper right\")\n",
    "  \n",
    "# rescaling the subplots\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "  \n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_data, fitted_lambda = stats.boxcox(clean['Rpm'])\n",
    "  \n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "print(fitted_lambda)\n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(clean['Rpm'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean[['Depth','ecd','GR', 'Res B','Res C','Res D','Rop','Rpm','Sppa','flow','tor','wob','JIF','HSI','Mse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2= clean_data[(clean_data != 0).all(1)]\n",
    "data_4 = data_2[(data_2['ecd']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log(data_4['ecd']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(data_4['GR']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fitted_data, fitted_lambda = stats.boxcox(data_4['GR'])\n",
    "  \n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "  \n",
    "# plotting the original data(non-normal) and \n",
    "# fitted data (normal)\n",
    "sns.distplot(data_4['GR'], hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Non-Normal\", color =\"green\", ax = ax[0])\n",
    "  \n",
    "sns.distplot(fitted_data, hist = False, kde = True,\n",
    "            kde_kws = {'shade': True, 'linewidth': 2}, \n",
    "            label = \"Normal\", color =\"green\", ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clean_data['ecd']).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.hist(figsize=(8,7),column= ['ecd','GR', 'Res B','Res C','Res D','Rop','Rpm','Sppa','tor','flow','wob','JIF','HSI','Mse']);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(train):\n",
    "    correlations = train.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(clean_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(corr)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaner_data[cleaner_data['flow']>500].plot.scatter(x='flow', y='HSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#print(data_33)\n",
    "#subset_clean_data = clean_data[['GR','DT','RHOB']]\n",
    "\n",
    "subset_data_3 = clean_data[['ecd','GR', 'Res B','Rop','Rpm','tor','flow','wob','HSI','Mse']]\n",
    "#print(subset_data_3)-=ll\n",
    "subset_data_3 = subset_data_3[(np.abs(stats.zscore(subset_data_3)) < 3).all(axis=1)]\n",
    "\n",
    "#print(subset_data_3)\n",
    "#clean_data = clean_data.merge(subset_clean_data,on=['GR','DT','RHOB'])\n",
    "#subset_data_3.shape\n",
    "\n",
    "data_3 = data_3.merge(subset_data_3,on=['ecd','GR', 'Res B','Rop','Rpm','tor','wob','flow','HSI','Mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "data = scaler.fit_transform(data_3) \n",
    "#print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drilling = data_3[['Rop','Rpm','tor','wob','HSI','flow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drilling_data = scaler.fit_transform(Drilling) \n",
    "#print (extractedData_scaled)\n",
    "#col_names = list(extractedData.columns)\n",
    "df_mm = pd.DataFrame(drilling_data, columns=['Rop','Rpm','tor','wob','HSI','flow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Formation = data_3[['Res B','GR','Mse']]\n",
    "Formation_data = scaler.fit_transform(Formation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "SSW = np.zeros(10) # Initialize the list SSW to be empty\n",
    "for k in range(1,10):\n",
    "    kmeans_for_data = KMeans(n_clusters=k, random_state=42).fit(Formation_data)\n",
    "    SSW[k] = kmeans_for_data.inertia_\n",
    "\n",
    "\n",
    "print('The SSW values are', SSW[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = range(1, 10)\n",
    "plt.ylabel('SSW')\n",
    "plt.xlabel('number of clusters')\n",
    "plt.plot(clusters, SSW[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', n_init = 10, max_iter = 300)\n",
    "electrofacies = kmeans.fit_predict(Formation_data)\n",
    "#electrofacies.predict(Formation_data)\n",
    "#_predict\n",
    "#plt.scatter(Formation_data[:, 0], Formation_data[:, 1], c=electrofacies, s=50, cmap='viridis')\n",
    "#plt.xlabel('RES')\n",
    "#plt.ylabel('GR')\n",
    "\n",
    "#centers = Formation_data.cluster_centers_\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='r', s=200);\n",
    "#sns.pairplot(Formation_data)\n",
    "#print(kmeans.labels_)\n",
    "#print(kmeans.cluster_centers_)\n",
    "#y_kmeans = kmeans.predict(extractedData_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm ['Facies'] = electrofacies\n",
    "print(electrofacies)\n",
    "sns.set(style = 'ticks') # Set the background to dark\n",
    "#sns.pairplot(data_3, vars=['ROP','RPM', 'standpipe','STOR','SWOB','flow','pb','vn','JIF','HSI'] ,hue = 'Facies') # Create a matrix scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['GR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['Res B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['Mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['tor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['wob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['Rop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_mm['Facies'], y=data_3['Rpm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_mm, vars = ['Rop','Rpm','tor','wob','flow'], hue = 'Facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RockType = df_mm['Facies'].to_numpy()\n",
    "Y = RockType\n",
    "X = drilling_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size = 0.2)\n",
    "#Y_train\n",
    "#Q = Y_train  #['Facies'].to_numpy()\n",
    "#W = Y_test\n",
    "#print (Q)\n",
    "#print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class0 = np.where(Y_test == 0)[0]\n",
    "i_class2 = np.where(Y_test == 2)[0]\n",
    "i_class1 = np.where(Y_test== 1)[0]\n",
    "n_class0 = len(i_class0)\n",
    "n_class2 = len(i_class2) \n",
    "n_class1 = len(i_class1) \n",
    "print (n_class2)\n",
    "print(n_class0)\n",
    "print(n_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0 = df_mm[(df_mm['Facies'] == 0)]\n",
    "zero = cluster0.to_numpy()\n",
    "#print(cluster0)\n",
    "cluster1 = df_mm[(df_mm['Facies'] == 1)]\n",
    "one = cluster1.to_numpy()\n",
    "#print (cluster1)\n",
    "cluster2 = df_mm[(df_mm['Facies'] == 2)]\n",
    "two = cluster2.to_numpy()\n",
    "#print(cluster2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just one Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mm = df_mm.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_mm[:, 0]\n",
    "X = df_mm[:, [1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'C': [1, 5], 'kernel':['rbf'], 'gamma': [5, 10, 50], 'epsilon': [0.001, 0.01, 0.005]}\n",
    "gsc = GridSearchCV(SVR(), param_grid = param, cv=5, verbose=3)\n",
    "grid_result = gsc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_pred = grid_result.predict(x_test)\n",
    "#y_predd = pd.DataFrame(y_pred, columns=[\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "#print(errors)\n",
    "from sklearn import metrics\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_mm[:, 0]\n",
    "X = df_mm[:, [1,2,3,4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'C': [1, 5], 'kernel':['rbf'], 'gamma': [5, 10, 50], 'epsilon': [0.001, 0.01, 0.005]}\n",
    "gsc = GridSearchCV(SVR(), param_grid = param, cv=5, verbose=3)\n",
    "grid_result = gsc.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " y_pred = grid_result.predict(x_test)\n",
    "#y_predd = pd.DataFrame(y_pred, columns=[\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "#print(errors)\n",
    "from sklearn import metrics\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)\n",
    "\n",
    "r2_score(y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predicted value to actual value of ROP \n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt \n",
    "_, ax = plt.subplots()\n",
    "#plt.figure(figsize=(20,20))\n",
    "ax.scatter(x=y_test, y = range(0, y_test.size), c = 'blue', label = 'Actual', alpha = 0.5)\n",
    "ax.scatter(x=y_pred, y = range(0, y_pred.size), c = 'red', label = 'Predicted', alpha = 0.5)\n",
    "plt.gcf().set_size_inches((10,7))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title ('actual and predicted ROP')\n",
    "plt.xlabel('ROP')\n",
    "plt.ylabel('Depth')           \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z= y_test.to_numpy()\n",
    "#w = y_pred.to_numpy()\n",
    "df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\n",
    "df00 = df.head(50)\n",
    "\n",
    "df00.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_mm[:, 0]\n",
    "X = df_mm[:, [1,2,3,4,5]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size = 0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': list(range(4, 10)), 'min_samples_leaf': [1,2,3,4,5], 'n_estimators':[500, 1000]}\n",
    "grid_search_cv = GridSearchCV(RandomForestRegressor(random_state=42), params, verbose=1, cv=5)\n",
    "\n",
    "grid_search_cv.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search_cv.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "#print(errors)\n",
    "\n",
    "r2_score(y_test, y_pred)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on train set: %f\" % grid_search_cv.score(x_train, y_train))\n",
    "\n",
    "print(\"Score on test set: %f\" % grid_search_cv.score(x_test, y_test))\n",
    "print(\"Best parameters: %s\" % grid_search_cv.best_params_)\n",
    "\n",
    "cv_results = pd.DataFrame(grid_result.cv_results_)\n",
    "cv_results_tiny = cv_results[['param_C', 'mean_test_score']]\n",
    "cv_results_tiny.sort_values(by='mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pd.DataFrame(X, columns = ['RPM','STOR','SWOB','flow','HSI'])\n",
    "\n",
    "feature_list = list(x0.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(grid_search_cv.best_estimator_.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x0: x0[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data + Facies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_mm[:, 0]\n",
    "X = df_mm[:, [1,2,3,4,5,6]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {'max_depth': list(range(4, 10)), 'min_samples_leaf': [1,2,3,4,5], 'n_estimators':[500, 1000]}\n",
    "grid_search_cv = GridSearchCV(RandomForestRegressor(random_state=42), params, verbose=1, cv=5)\n",
    "\n",
    "grid_search_cv.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.datasets import make_regression\n",
    "\n",
    "#regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "#regr.fit(X, y)\n",
    "#RandomForestRegressor(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search_cv.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "#print(errors)\n",
    "\n",
    "r2_score(y_test, y_pred)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on train set: %f\" % grid_search_cv.score(x_train, y_train))\n",
    "\n",
    "print(\"Score on test set: %f\" % grid_search_cv.score(x_test, y_test))\n",
    "print(\"Best parameters: %s\" % grid_search_cv.best_params_)\n",
    "\n",
    "cv_results = pd.DataFrame(grid_result.cv_results_)\n",
    "cv_results_tiny = cv_results[['param_C', 'mean_test_score']]\n",
    "cv_results_tiny.sort_values(by='mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pd.DataFrame(X, columns = ['RPM','STOR','SWOB','flow','HSI', 'Facies'])\n",
    "\n",
    "feature_list = list(x0.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(grid_search_cv.best_estimator_.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x0: x0[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model0 = grid_result\n",
    "class Simulate:\n",
    "    def __init__(self, obs, var):\n",
    "        self.obs = obs\n",
    "        self.var = var\n",
    "\n",
    "    def simulate_increase(self, model0, percentage):\n",
    "        baseline = model0.predict(self.obs)\n",
    "        plus = {}\n",
    "        for ivar in self.var:\n",
    "            X_plus = self.obs.copy()\n",
    "            X_plus[ivar] = X_plus[ivar] + X_plus[ivar] * (percentage / 100)\n",
    "            plus[ivar] = model0.predict(X_plus)\n",
    "        b = pd.DataFrame(\n",
    "            plus, index=['simulated'\n",
    "                         ]).T.reset_index().rename(columns={'index': 'test'})\n",
    "        b['baseline'] = baseline[0]\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_simulation(d, **kwargs):\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.barplot(x='test', y='simulated', data=d, palette='deep', ax=ax)\n",
    "        ax.axhline(d['baseline'].values[0], color='grey', linestyle='--', linewidth=2)\n",
    "        ax.plot([0, 0], [-100, -100], color='grey', linestyle='--', linewidth=2, label='baseline')\n",
    "\n",
    "        #maxi = int(d['simulated'].max() + d['simulated'].max() * 0.1)\n",
    "        #mini = int(d['simulated'].min() - d['simulated'].min() * 0.11)\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "        ax.set_xlabel('Simulated variables')\n",
    "        ax.set_ylabel('Target value')\n",
    "        ax.set_title(kwargs.get('title'))\n",
    "        ax.legend()\n",
    "\n",
    "        ax.grid(axis='y', linewidth=.3)\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "#xx = X[:, [1,2,3,4,5]]     \n",
    "x_0 = pd.DataFrame(X, columns = ['RPM','STOR','SWOB','flow','HSI', 'Facies'])\n",
    "\n",
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "PERC = 80\n",
    "ROW = x_0.iloc[[10]]\n",
    "S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "print(d)\n",
    "S.plot_simulation(d, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value')\n",
    "\n",
    "\n",
    "perc_change = ((d['simulated']-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "d['perc_change_%'] = perc_change\n",
    "\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x_0.loc[x_0.Facies == 0,:]\n",
    "w = x_0.loc[x_0.Facies == 1,:]\n",
    "e = x_0.loc[x_0.Facies == 2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate:\n",
    "    def __init__(self, obs, var):\n",
    "        self.obs = obs\n",
    "        self.var = var\n",
    "\n",
    "    def simulate_increase(self, model0, percentage):\n",
    "        baseline = model0.predict(self.obs)\n",
    "        plus = {}\n",
    "        for ivar in self.var:\n",
    "            X_plus = self.obs.copy()\n",
    "            X_plus[ivar] = X_plus[ivar] + X_plus[ivar] * (percentage / 100)\n",
    "            plus[ivar] = model0.predict(X_plus)\n",
    "        b = pd.DataFrame(\n",
    "            plus, index=[percentage\n",
    "                         ]).T.reset_index().rename(columns={'index': 'test'})\n",
    "        b['baseline'] = baseline[0]\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_simulation(d, y, **kwargs):\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.barplot(x='test', y=y, data=d, palette='deep', ax=ax)\n",
    "        ax.axhline(d['baseline'].values[0], color='grey', linestyle='--', linewidth=2)\n",
    "        ax.plot([0, 0], [-100, -100], color='grey', linestyle='--', linewidth=2, label='baseline')\n",
    "\n",
    "        #maxi = int(d['simulated'].max() + d['simulated'].max() * 0.1)\n",
    "        #mini = int(d['simulated'].min() - d['simulated'].min() * 0.11)\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "        ax.set_xlabel('Simulated variables')\n",
    "        ax.set_ylabel('Target value')\n",
    "        ax.set_title(kwargs.get('title'))\n",
    "        ax.legend()\n",
    "\n",
    "        ax.grid(axis='y', linewidth=.3)\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "#facies_0 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range (-100, 150, 50):\n",
    "    PERC = i\n",
    "    ROW = z.iloc[[100]]\n",
    "    S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "    d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "    perc_change = ((d[i]-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "    d['perc_change_%'] = perc_change\n",
    "    S.plot_simulation(d, i, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value for facies 0')\n",
    "    print (d)\n",
    "\n",
    "#PERC = 80\n",
    "#ROW = z.iloc[[5]]\n",
    "#S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "#d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "\n",
    "#print(d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for \n",
    "\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data.txt\", \"w\")\n",
    "#str_dictionary = repr(a_dictionary)\n",
    "file.write(d)\n",
    "#\"\\n\" creates newline for next write to file\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "#facies_0 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range (-100, 150, 50):\n",
    "    PERC = i\n",
    "    ROW = w.iloc[[10]]\n",
    "    S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "    d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "    perc_change = ((d[i]-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "    d['perc_change_%'] = perc_change\n",
    "    S.plot_simulation(d, i, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value for facies 1')\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "#facies_0 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range (-100, 150, 50):\n",
    "    PERC = i\n",
    "    ROW = z.iloc[[10]]\n",
    "    S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "    d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "    perc_change = ((d[i]-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "    d['perc_change_%'] = perc_change\n",
    "    S.plot_simulation(d, i, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value for facies 2')\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model0 = grid_result\n",
    "class Simulate:\n",
    "    def __init__(self, obs, var):\n",
    "        self.obs = obs\n",
    "        self.var = var\n",
    "\n",
    "    def simulate_increase(self, model0, percentage):\n",
    "        baseline = model0.predict(self.obs)\n",
    "        plus = {}\n",
    "        for ivar in self.var:\n",
    "            X_plus = self.obs.copy()\n",
    "            X_plus[ivar] = X_plus[ivar] + X_plus[ivar] * (percentage / 100)\n",
    "            plus[ivar] = model0.predict(X_plus)\n",
    "        b = pd.DataFrame(\n",
    "            plus, index=['simulated'\n",
    "                         ]).T.reset_index().rename(columns={'index': 'test'})\n",
    "        b['baseline'] = baseline[0]\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_simulation(d, **kwargs):\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.barplot(x='test', y='simulated', data=d, palette='deep', ax=ax)\n",
    "        ax.axhline(d['baseline'].values[0], color='grey', linestyle='--', linewidth=2)\n",
    "        ax.plot([0, 0], [-100, -100], color='grey', linestyle='--', linewidth=2, label='baseline')\n",
    "\n",
    "        #maxi = int(d['simulated'].max() + d['simulated'].max() * 0.1)\n",
    "        #mini = int(d['simulated'].min() - d['simulated'].min() * 0.11)\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "        ax.set_xlabel('Simulated variables')\n",
    "        ax.set_ylabel('Target value')\n",
    "        ax.set_title(kwargs.get('title'))\n",
    "        ax.legend()\n",
    "\n",
    "        ax.grid(axis='y', linewidth=.3)\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "#xx = X[:, [1,2,3,4,5]]     \n",
    "x_0 = pd.DataFrame(X, columns = ['RPM','STOR','SWOB','flow','HSI', 'Facies'])\n",
    "\n",
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "PERC = 80\n",
    "ROW = w.iloc[[5]]\n",
    "S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "d = S.simulate_increase(model0=grid_search_cv, percentage=PERC)\n",
    "print(d)\n",
    "S.plot_simulation(d, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value')\n",
    "\n",
    "\n",
    "perc_change = ((d['simulated']-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "d['perc_change_%'] = perc_change\n",
    "\n",
    "\n",
    "print(d)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_OPTIMIZE = ['SWOB','HSI','RPM', 'flow']\n",
    "#print (VAR_OPTIMIZE)\n",
    "PERC = 80\n",
    "ROW = e.iloc[[5]]\n",
    "S = Simulate(obs=ROW, var=VAR_OPTIMIZE)\n",
    "d = S.simulate_increase(model0=grid_result, percentage=PERC)\n",
    "\n",
    "print(d)\n",
    "\n",
    "S.plot_simulation(d, title=f'Impact of a {PERC}% increase of {VAR_OPTIMIZE} in target value for facies 2')\n",
    "\n",
    "\n",
    "perc_change = ((d['simulated']-d['baseline'])/d['baseline']) *100\n",
    "\n",
    "d['perc_change_%'] = perc_change\n",
    "\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = pd.DataFrame(X, columns = ['RPM','STOR','SWOB','flow','HSI', 'Facies'])\n",
    "\n",
    "feature_list = list(x0.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(grid_search_cv.best_estimator_.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x0: x0[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(y_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(errors.flatten())\n",
    "df.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
